
\subsection{Requirements Elicitation and Discovery}

The manual process of requirement elicitation is expensive in terms of
effort and resources. A project's success majorly depends
on the precise identification of stakeholder's expectations and requirements for
the system they desired.
A possibility to do requirements elicitation is to mine available datasets e.g.
social media, requirement documents, app stores reviews \levi{which documents?}. The latest trend for
identifying user requirements is to mine data coming from social media e.g. twitter or
google store or app stores. These user reviews are not structured requirements
and contain information such as praise, dislikes or bug reports. Also, have extra information and noise that make manual requirements elicitation a difficult and challenging task.
Automated requirement elicitation is helpful in these cases and can
significantly reduce time, effort, and cost. This is mainly an ML
\emph{classification} task: give the set of information and identifying it as a
requirement or not. Sometimes \emph{clustering} is also used for auxiliary
tasks.\\

Guzman \etal \cite{Guzman:2017} have proposed the ALERTme approach for
classifying, grouping and ranking tweets during the software
evolution development. Many users posts about software on twitter and. it is hard to manually identify tweets as a requirement or not. The proposed methodology classify tweets as improvement requests or not, using \emph{Naïve Bayes} algorithm. This was the first study of
its kind performed on software tweets. 
The classifier was trained with following steps: 1) conversion of
preprocessed tweets into a VSM model, 2) train a classifier
on a set of manually annotated tweets, 3) predict the tweets categories using the trained classifier. Furthermore, improvement
requests are considered for the grouping which helped to sort the
request and summarize them accordingly. The summarization process contains highly ranked tweets based on likes, sentiment, share etc parameters.

Williams \etal \cite{Williams:2017} performed a similar study on tweets in order
to classify them as user requirements or not. The study improved results in
comparison to the existing studies using Naïve Bayes and VSM algorithm. For the learning
process, manually annotated(labelled) tweets were used\levi{annotated how, which ML
algorithm was used?}. The authors claim on bases of results that
software tweets are neutral in nature, meaning sentiment analysis did not
influence the outcome of the ML algorithm. It shows improved results in comparison to \cite{Guzman:2017}.
The study used 4000 randomly selected tweets from ten different
software including Visualstudio, chrome, instagram etc\levi{software what?}.\\

Jiang \etal \cite{Jiang:2014} mined user reviews in mobile application
(app) stores for discovering evolutionary requirements. The author's method
clustered opinion expressions in the form of a macro network
topology\levi{explain what kind of clustering}, and combines polarized
sentimental analysis with economic factors to decide evolutionary
requirements.\levi{explain} It used two datasets of online reviews: one from the
Karplersky internet security 2011 software package (from Amazon) with 380
reviews; the other one  comprising 461 reviews for the  TuneIn Radio Pro V3.6
mobile app (from the app store).\\

Lange \etal\cite{Douglas:S2008} mapped the software requirement elicitation
process onto an military tool \emph{skiweb} in order to make their organizational 
decision\levi{which decision?}. The proposed methodology used supervised
\emph{Naive Bayes} algorithm to classify the text document to find related
requirements to thegiven discussion topic \levi{related to what?}. Furthermore, this recommender
system\levi{which recommender system?} used topic modeling to identify the key
stakeholder for whom that requirement is important and allow for further
analysis\levi{which analysis?}. This study used an internal organizational
dataset Skiweb Data i.e. wiki, blogs etc.\\

Jha \etal \cite{Jha:2017} discover user requirements by mining feature requests
submitted to the app store. The requests were classified into three categories:
\emph{bugs}, \emph{features}, and \emph{junk}. The authors have used SVM and
Naïve Bayes as classifiers. The study used frame semantic analysis that
generalize more general contexts\emph{explain frame semantics}. This technique
produced a slower dimensional model\levi{why slower?} with a smaller number of
features which help to enhance the prediction capabilities. It combined existing
datasets from past studies and reviews for iOS apps including CreditKarma,
FitBit, and Gmail.

Maalej presents in \cite{Maalej} a study on how to classify app reviews as bug
reports, feature requests, user experiences, and ratings. The binary classifier
\emph{Naive Bayes} performed better than other algorithms also it showed binary
classifier are suitable than multi-class
classifiers. A binary classifier only consider boolean value i.e. yes or no. Different reviews feature such as ratings,
tense, sentiment scores, and length enhanced the classification performance. A
dataset of 4400 manually annotated reviews from Google Play and the Apple app
store was used for the study.\\

Herrera \etal \cite{Castro-Herrera:2009} have built a semi-automated\levi{why
semi-automated?} recommender system to manage broad stakeholder participation in
the requirements elicitation and prioritization process. On these forums,
stakeholders can work collaboratively to transform statements of need into sets
of articulated and prioritized requirements. The proposed system analyzes online
datasets\levi{which datasets?} gathered from stakeholders and automatically
generates specialized topics for building discussion forums. User profiles are
created as a next step, that depicts stakeholders interest.\levi{don't get this
sentence} These profiles also help to create recommendations according to the
interests of a community of similar stakeholders.\levi{is this the recommender
part?} For identifying topics, an unsupervised clustering technique is
applied to unstructured or semi-structured data\levi{what's the difference?}.
The evaluation  dataset was a collection of 36 feature requests created by
graduate-level students for an Amazon-like student web-portal system.\\




