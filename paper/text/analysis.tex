\subsection{Requirements Specification and Analysis}

Software requirements specifications are usually stated in informal, imprecise
and ambiguous natural language, making analyzing them a challenging task.
The success of a system does not solely depend on its functional requirements,
but also significantly relies on adherence to non-functional requirements.
The primary focus of requirements analysis is generally towards the
identification and specification of \FRs. \NFRs are usually identified and
specified in later development stages, which can increase the risks of problems
during the development lifecycle. Non-functional requirements may not be
mentioned explicitly in a formal specification requirements documents, even
though they exist for all systems in freeform documents like interview notes or
meeting minutes. All types of requirements are analyzed differently, and as
such, it is useful to separate them. This distinction helps in managing changes in requirements as
well as in precisely incorporating them in the development of the system. Manual
requirement division into \FRs and \NFRs is difficult and time-consuming.
Machine learning can be useful in supporting analysts in the error-prone task of
manually discovering and classifying them such requirements, having in mind
easing further analysis tasks.

\subsubsection{Identifying Non-Functional Requirements}
\tahira{Non-functional requirements may not be explicitly mentioned in a formal
specification requirements documents even though they exist for all
systems~\cite{Slankas:2013}. Moreover, freeform documents like interview notes,
meeting minutes and scattered requirements specifications include non-functional
requirements which need to be detected and classified. \ML can be
useful to support analysts in the error-prone task of manually discovering and
classifying \NFR. Automatic detection can be used to quickly and effectively
identifying \NFR in large and complex documents \cite{Cleland-Huang2007}}
This is a classification problem, as from a set of requirements we want to
decide whether or not requirements are \NFRs. 

In a study by Slankas \etal~\cite{Slankas:2013}, the authors automatically
identify and classify sentences in natural language from user agreements, install
manuals, regulations, requirements specifications and user manuals into 14
different \NFR categories, among which \emph{Access Control}, \emph{Audit},
\emph{Availability} or \emph{Legal}.
\tahira{\emph{Access Control}, \emph{Audit}, \emph{Availability},  and \emph{Legal} etc.
\emph{Look and Feel}, \emph{Maintenance}, \emph{Operational}, \emph{Privacy},
\emph{Recoverability}, \emph{Performance and Scalability}, \emph{Reliability},
\emph{Security} and \emph{Usability}.}
The authors' two-step process is as follows: 1) parsing natural language and
turn sentences into graphs\levi{which kind of graphs?}; 2) classifying sentences
into categories with k-nearest neighbor algorithm led the authors to finding 20
keywords for each category of \NFR as features for their classifier.
They then trained the \NFR classifier with a wide variety of open and closed
source EHRs (Electronic Health Record), various industry standards (HL7, CCHIT)
and governmental regulations.

\tahira{Cleland-Huang \etal~\cite{Cleland-Huang2007} explored a similar approach and
used the k-nearest neighbor classifier for grouping \NFR e.g.
\emph{availability}, \emph{look-and-feel}, \emph{legal}, \emph{maintainability},
\emph{operational}, \emph{performance}, \emph{scalability}, \emph{security}, and
\emph{usability}}
Cleland-Huang \etal~\cite{Cleland-Huang2007} explored a similar approach and
used the k-nearest neighbor classifier for grouping \NFRs e.g.
\emph{availability}, \emph{look-and-feel}, \emph{legal}. For training their
classifier, they used 15 requirements specifications developed as term projects
by master students at DePaul University.
\subsubsection{Identifying Functional Requirements}
\tahira{Software requirements specifications are usually stated in informal,
imprecise and ambiguous natural language, thus analyzing them is a challenging
task. However, for satisfying the specifications of a product analyzing
functional requirements is a vital task.Automatically extract structured
information of functional requirements from Software Requirements Specifications
and grouping them into different categories is a \ML classification
task\cite{7949577}.}This is a classification problem as from a set of
requirements we want to decide whether or requirements are \FRs.
\tahira{Wang \etal~\cite{7949577} applied a combination of \ML, natural
language processing, and semantic analysis to automatically extract functional
requirements and classify them into ten different cases: Agentive (The main
participant of activities), \emph{action} (the main functional operation),
 \emph{objective} (the objects affected by the Action), \emph{agent mode} (the
 modifier or property of the Agentive), \emph{objmod} (the modifier or property
 of the Agentive), \emph{locational} (the location or
 destination of an Action), \emph{temporal} (the occurrence time or frequency of
 an Action), \emph{manner} (the way or tool by which an Action is performed),
 \emph{goal} (the purpose to be achieved through the Action) and
 \emph{constraint} (the conditions or constraints of the \emph{action}). Their
 framework employed techniques of semantic role labeling (which assigns a 
role  label  for  each  word  in  the  sentence) and machine learning and has four steps: corpus construction, NLP
 preprocessing, feature extraction and EFRF (Extended Functional Requirements
 Frame) functional cases extraction. \emph{Bring to the preprocessing section: For NLP processing they did
 tokenization, lemmatization, part-of-speech tagging (POS tagging) and
 dependency parsing.}}

Wang \etal~\cite{7949577} applied a combination of \ML, natural language
processing, and semantic analysis to automatically extract functional
requirements and classify them into different types such as action, objective,
goal, temporal or constraints. Their framework employed techniques of semantic
role labeling (which assigns a role label for each word in the sentence) and
 \ML is applied through four steps: corpus construction, NLP preprocessing,
 feature extraction and EFRF (Extended Functional Requirements Frame) functional
 cases extraction.
 The authors trained their bi-directional LSTM-CRF network, a variant of
 Recurrent Neural Networks architecture model, with an E-commerce requirements
 dataset and tested it on the requirements of automaker systems. They proposed
 EFRF through analyzing the linguistic characterization of software requirement
 specifications. EFRF consists of the 10 \FR types and allows capturing the
 semantic information in natural language. \tahira{The authors have shown that
 their model trained on an e-commerce requirements dataset can be used to
 classify functional requirements and extract semantic information from the
 requirements of automaker systems.}
\subsubsection{Distinguishing Functional from Non functional Requirements}

\tahira{The success of a system does not solely depend on its functional requirements.
Like for functional requirements, it also significantly depends on the adherence
to non-functional requirements. The primary focus of requirements analysis is
generally towards the identification and specification of \FR. \NFR are usually
identified and specified in later development stages which can increase the
risks of problems during the development lifecycle. \FR tend to be more
straightforward e.g. store and retrieve data when a user performs some specific action\levi{transaction?}. On the
other side, \NFR are complicated and challenging to implement for example making the
design to meet the \NFR or design test cases for \NFR\levi{unclear}.
Different types of requirements are analyzed differently, and as such, it is
useful to separate them. This distinction helps in managing changes in
requirements as well as in precisely incorporating them in the development of
the system. Manually dividing requirements in \FR and \NFR is difficult and
time-consuming. \ML can be used for reducing the effort and
categorizing the requirements based on the text segment analysis\emph{what is
this?}This is \ML classification task: give the set of requirements and
identifying its category.}This
is a classification problem as from a set of requirements we want to decide
whether or not requirements belong to a certain class. 

Lu \etal~\cite{Lu:2017} automatically classified text from user reviews for
(app) stores into \FRs, \NFRs. The authors further classified \NFRs
into four categories: reliability, usability, portability, and
performance. The approach used a supervised algorithm called
\emph{bagging}.\levi{is bagging an algorithm?} \tahira{ yes it is from ensemble
learning training the classifier.}\levi{I think there is a conflict with the
same word in the preprocessing text.} The text trimming used standard
pre-processing steps. Also, the sentences were augmented by several most similar
words to the user reviews in the training set and showed that augmented user
reviews can lead to better classification results.\levi{unclear} %The machine
learning algorithm,% Bagging was more suitable for \NFR classification in the
proposed study than Na√Øve Bayes and J48\levi{is this conclusion because they
tested with multiple algorithms or because it is better than the SOA, in which
case this should be mentioned in the table} \tahira{ they tried two classifier
Naive Bayes and J.48 and results showed comparison of these two}. The authors
used 6696 raw user reviews from iBooks and 4400 raw user reviews from WhatsApp.

Deoxadez \etal~\cite{Deocadez:2017} used semi-supervised classification
techniques for automated classification of \FR and \NFR from user reviews from
the app store. This study dealt with two problems: 1) minimizing the annotation
effort for labeling the big dataset of user reviews, and 2) classification of
\FR and \NFR. The proposed solution to the first problem used the
semi-supervised self-labeling algorithm. Self-labeling algorithms required a
small amount of dataset to get comparable results as supervised techniques.
Features were obtained by applying the standard pre-processing and the \BOW
algorithm. For user reviews the \emph{Naive Bayes} classification algorithm was
selected because of its general high-performance results in classification
problems. It used reviews of top 40 paid and free apps from ten
categories.\levi{categories of what?}

Kutranvoic \etal ~ \cite {Kurtanovic:2017} performed automated analysis on
software requirement documents written in natural language in order to classify
them as \FRs, \NFRs and subcategories of \NFRs. The basic pre-processing steps
were operated on the requirements before applying the ML algorithm. The authors
used the \SVM algorithm for the classification.
Some \NFR were ignored because of their minor presence in the documents. Also,
the dataset was imbalanced that meant it was not equally distributed.\levi{don't
get these last 2 sentences} For avoiding data imbalance problem additional
dataset i.e. user comments\levi{what is this?} from Amazon was integrated into
the main dataset.
The study used data from open source tera PROMISE repository that consists of
625 labeled natural language requirements (255 \FR [40.8\%] and 370
\NFR[59.2\%]).

Abad \etal~\cite{Abad:2017} targeted two similar problems: the first one is the
classification of \FR and \NFR, and the second classification of \NFR into
categories.The authors first performed pre-processing for text trimming.
Afterwards they increased the weight of influential words in the dataset using
feature co-occurrence and regular expression. They then used the decision tree
(\DT) J.48 algorithm for classification of \FRs and \NFR.
Additionally, Binarized Naive Bayes (BNB) was used for sub-classification of
\NFR. \BNB outperformed among other algorithms such as clustering, k-means, and
hybrid clustering. The study showed that the text pre-processing approach
positively influenced on classification.\levi{unclear} It improved \FR and \NFR
 classification accuracy from \textit{89.92\% to 95.04\%}. It used reviews of
 top 40 paid and free apps from ten
different categories.

Garzoli \cite {Garzoli:2013} proposed a system for the analysis of requirements
that allowed identifying software functionalities within large collections of
requirements written in natural language. It classified the large dataset into
five types: \FRs, \NFRs, \emph{design and construction constraints},
\emph{operator requirements} and \emph{performance requirements}.  The goal of
the study was to come up with a general architecture for large-scale and
adaptive requirement analysis. It used \BOW together with text mining techniques
for lexical and grammatical feature for information retrieval and \SVM for the
classification of requirements. The learning classifiers\levi{?} contained 4,727
annotated requirements, related to three different scenarios. The dataset was
taken from the Naval Combat Managment System.\levi{what does the dataset consist
of?}

Wieloch \etal presented in~\cite{Wieloch:2013} Trace by Classification (TBC), an
\ML approach in which a classifier is trained to identify and classify requirements
and/or other kinds of software artifacts which occur relatively frequently
across different projects (they call this process generating trace links for
software artifacts in their research).\levi{Trivial preprocessing step put in
preprocessing section:}
The first step in their approach was preprocessing to eliminate common stop
words. Then, in the training phase identified a set of indicator
terms\levi{features?} for each \NFR category and the classifier trained by the
set of identified weighted indicator terms that can be used for the last step
which is to classify additional artifacts into functional and non-functional
(e.g. look-and-feel, performance, security, etc). A probability value
represented a possibility that the new requirement belonged to a certain
artifact type computed as a function of the occurrence of indicator terms of
that type in the requirement.\levi{shouldn't this go to the
verification/traceability section?} \tahira{Parisa added this paper and
according to her it just contained tracability and by tracability they mean
classification}
\levi{shouldn't this go to the verification/traceability section?} \tahira{Parisa added this paper and according to her it just contained tracability and by tracability they mean classification}

\subsubsection{Requirement Prioritization}
Complex software system generally has thousands of requirements with multiple
stakeholders and customers. Each one of them has their own set of requirements
and opinions and wants their requirements implementation accordingly. However,
factors such as budget or different opinions among stakeholdersmake often make
implementating all the requirements a complicated task. Therefore, it is
important to make a proper decision for prioritizing requirements considering
all the factors that are important for the success of the project. Different
models exist in the literature for prioritizing software requirements, among
which analytical hierarchical process (AHP) \cite{saaty2008}, Goal oriented
\cite{VanLamsweerde:2001} or the cost value approach \cite{Karlsson:1997}. In
these techniques, human input is very important. Qaddoura \etal
\cite{R.Qaddoura} reviewed  prioritization techniques and also shed light on the
contribution of \ML to the topic. \ML can be used for automated analysis of
these large set of software requirements prioritization, as well as in helping
to improve existing techniques.

Dhingra \etal  \cite{S.Dhingra} predicted the most appropriate\levi{why the
most appropriate?} technique for software requirement prioritization process.
The input from the user was taken as characteristic values (detect consistency, maintain information, not
available, or both) for different attributes. These attributes list included
consistency, traceability, priority basis, rigorous/systematic, distributed
stakeholder, cognitive aspects, and human experience. The output was the most
appropriate requirement prioritization method e.g. AGORA, AHP etc. The proposed
system framework had three phases; training phase, fuzzing inference process,
and testing phase. The drawback of fuzzy approach was the wrong prediction for
boundary values, which was overcomed by adopting decision trees. \DT learned
from datasets and predicted the most suitable prioritization technique. Out of
45 test samples, the framework classified 43 tests accurately.

Avesani \etal presented in \cite{PAvesani} a study that dealt with the
scalability problems which arise in managing the prioritization of a large number of
requirements when using the AHP technique. The existing solution to
scalability issues used heuristics\levi{which ones?}. It helped to decide when
to stop the pairwise elicitation process. The proposed framework outperformed AHP by giving
an accurate approximation of the final ranking while restricting the elicitation
effort. It used a rank-based learning algorithm and produced a ranking of all
requirements. This technique built up the solution by looking at
examples\levi{remove?}. The input for the learning algorithm were a finite set
of requirements, the ranking criteria, initial user preferences and density function.

A similar study was performed in \cite{7320432} by the same group
\cite{PAvesani} for identifying decision-making issues related to the
management of risks in Open Source Software adoption in medium and large
organizations. A semi-automated system was proposed that used case based
ranking classification algorithm. The input was priority elicitation of goals by
the decision maker and risk goal ranking function (predefined ranking criteria
ordering the goal). As an output, it ranked the final risk-based
goals.\levi{revise text - goals are requirements?}

\input{text/analysis_security}

